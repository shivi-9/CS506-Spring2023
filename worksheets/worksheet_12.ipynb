{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 12\n",
    "\n",
    "Name: Shivangi\n",
    "UID: U35642613\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Naive Bayes\n",
    "- Model Evaluation\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "| Attribute A | Attribute B | Attribute C | Class |\n",
    "|-------------|-------------|-------------|-------|\n",
    "| Yes         | Single      | High        | No    |\n",
    "| No          | Married     | Mid         | No    |\n",
    "| No          | Single      | Low         | No    |\n",
    "| Yes         | Married     | High        | No    |\n",
    "| No          | Divorced    | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| Yes         | Divorced    | High        | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "\n",
    "a) Compute the following probabilities:\n",
    "\n",
    "- P(Attribute A = Yes | Class = No)\n",
    "- P(Attribute B = Divorced | Class = Yes)\n",
    "- P(Attribute C = High | Class = No)\n",
    "- P(Attribute C = Mid | Class = Yes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(Attribute A = Yes | Class = No) : 3/7\n",
    "- P(Attribute B = Divorced | Class = Yes) : 1/3\n",
    "- P(Attribute C = High | Class = No) : 3/7\n",
    "- P(Attribute C = Mid | Class = Yes) : 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Classify the following unseen records:\n",
    "\n",
    "- (Yes, Married, Mid) = P(Yes) * P(Yes | C = Y) * P(Married | C = Y) * P(Mid | C = Y) vs P(No) * P(Yes | C = N) * P(Married | C = N) * P(Mid | C = N)\n",
    "    = 3/10 * 0 * 0 * 1 vs 7/10 * 3/7 * 4/7\n",
    "    = No\n",
    "- (No, Divorced, High) = P(Yes) * P(No | C = Y) * P(Divorced | C = Y) * P(High | C = Y) vs P(No) * P(No | C = N) * P(Divorced | C = N) * P(High | C = N)\n",
    "    = 3/10 * 1 * 1/3 * 0 vs 7/10 * 4/7 * 1/7 * 3/7\n",
    "    = No\n",
    "- (No, Single, High) = P(Yes) * P(No | C = Y) * P(Single | C = Y) * P(High | C = Y) vs P(No) * P(No | C = N) * P(Single | C = N) * P(High | C = N)\n",
    "    = 3/10 * 1 * 2/3 * 0 vs 7/10 * 4/7 * 2/7 * 3/7\n",
    "    = No\n",
    "- (No, Divorced, Low) = P(Yes) * P(No | C = Y) * P(Divorced | C = Y) * P(Low | C = Y) vs P(No) * P(No | C = N) * P(Divorced | C = N) * P(Low | C = N)\n",
    "    =3/10 * 1 * 1/3 * 0 vs 7/10 * 4/7 * 1/7 * 3/7\n",
    "    = No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "a) Write a function to generate the confusion matrix for a list of actual classes and a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 3], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "actual_class = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predicted_class = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    classes = set(actual + predicted)\n",
    "    matrix = [[0 for _ in classes] for _ in classes]\n",
    "    counts = Counter(zip(actual, predicted))\n",
    "    for i, actual_class in enumerate(classes):\n",
    "        for j, predicted_class in enumerate(classes):\n",
    "            matrix[i][j] = counts[(actual_class, predicted_class)]\n",
    "    return matrix\n",
    "\n",
    "print(confusion_matrix(actual_class, predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume you have the following Cost Matrix:\n",
    "\n",
    "|            | predicted = Y | predicted = N |\n",
    "|------------|---------------|---------------|\n",
    "| actual = Y |       -1      |       5       |\n",
    "| actual = N |        10     |       0       |\n",
    "\n",
    "What is the cost of the above classification?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that takes in the actual values, the predictions, and a cost matrix and outputs a cost. Test it on the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_cost(confusion_matrix, cost_matrix):\n",
    "    cost = 0\n",
    "    for i in range(len(confusion_matrix)):\n",
    "        for j in range(len(confusion_matrix)):\n",
    "            cost += confusion_matrix[i][j] * cost_matrix[i][j]\n",
    "    return cost\n",
    "\n",
    "cal_cost(confusion_matrix(actual_class, predicted_class), [[-1, 5], [10, 0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the difference between a testing set and a validation set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A testing set is used to evaluate the performance of the model after training. This set of data is not used during training, and its purpose is to estimate the generalization error of the model, which is the error that would be expected on new, unseen data. The testing set is used to measure the model's accuracy, precision, recall, F1-score, etc.\n",
    "\n",
    "A validation set is used to tune the model's hyperparameters, such as the regularization parameter, learning rate, or number of hidden layers in a neural network. The validation set is used to estimate the performance of the model on new data that was not used during training or testing. The model is trained on the training set, evaluated on the validation set, and the hyperparameters are adjusted to improve the performance on the validation set. The best model is selected based on its performance on the validation set and then evaluated on the testing set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) What are some things you can do to handle the case where you have a highly imbalanced dataset (i.e. there are way more of one class than another). Describe both how you can provide better visibility into the failures of the model and how you can set your model training up for success."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods such as bagging, boosting, and stacking can help improve the performance of the model on imbalanced datasets. By combining multiple models or resampling techniques, ensemble methods can reduce the bias towards the majority class.\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble learning method that combines multiple models trained on different random subsets of the training data to reduce overfitting and improve the stability and accuracy of the predictions. Bagging involves sampling the training data with replacement to create multiple \"bootstrap\" samples, and then training a separate model on each sample. The predictions of the individual models are then combined using majority voting (for classification) or averaging (for regression) to produce the final prediction.\n",
    "\n",
    "Boosting is another ensemble learning method that iteratively trains weak models and combines them to produce a strong model. Unlike bagging, boosting focuses on sequentially improving the accuracy of the model by adjusting the weights of the training samples based on their importance. In each iteration, the algorithm trains a weak model on the current weighted samples and updates the weights based on the misclassification rate. The process is repeated until the error rate is minimized or a predefined maximum number of iterations is reached. The final prediction is the weighted sum of the predictions of the individual models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
